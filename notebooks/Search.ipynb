{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search\n",
    "\n",
    "__Question-Answering system:__\n",
    "* extract words\n",
    "* look up words in keyword database\n",
    "* extract similar keywords\n",
    "* for all these keywords, extract sentenceIDs. These are the candidates.\n",
    "\n",
    "\n",
    "Ranking:\n",
    "* If a sentence occurs more than once in that list, it contains more keywords, so start with that sentence. If not or if tie, start with sentence with highest score. OBS: score has to be 0.5 at least, otherwise the sentence is too likely to not be informative.\n",
    "* Eliminate all sentences from candidates that are too similar to start sentence. \n",
    "* Find sentence with highest score among leftovers.\n",
    "* Repeat until the desired length is reached or if there are no more candidates left.\n",
    "\n",
    "\n",
    "OOV in question:\n",
    "* lookup 10 most similar words in fasttext model, and check whether these words are in the keyword database.\n",
    "\n",
    "Special types of questions:\n",
    "* Who wrote X/about X?\n",
    "* What does X say about Y?\n",
    "\n",
    "The first kind has to return authors + title + 'ling.auf.net/'+url. Lookup X in the keyword list in the paper database. Possibly: my keyword list is raw, exact matches will be needed. An idea to get better results is to run the keywords list through is_english_sentence.  \n",
    "The second kind has to look up author in papers db, identify the papers with the right keywords, and do the search on the sentences from those papers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from gensim.models.wrappers import FastText\n",
    "from sklearn.externals import joblib\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, IndexedRow, IndexedRowMatrix\n",
    "from pyspark import SparkContext\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standard_stopwords = set(list(filter(lambda x: x not in ['who', \n",
    "                                                         'what', 'which', 'when', 'where', 'how', 'why'], \n",
    "                                     list(ENGLISH_STOP_WORDS)+list(stopwords.words('english')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in ['who', 'what', 'which', 'when', 'where', 'how', 'why']:\n",
    "    if w in standard_stopwords:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authors = joblib.load('authors')\n",
    "#fasttext = FastText.load_fasttext_format('../fastText/wiki.en.bin')\n",
    "bigrams = joblib.load('bigrams_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureVec = featureVec = joblib.load('featurevec')\n",
    "id_s = joblib.load('sentence_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({k:v for k, v in zip(id_s, featureVec)})\n",
    "df = pd.DataFrame(featureVec, index = id_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59a85acdb18b146ddb84ff2b</th>\n",
       "      <td>0.016841</td>\n",
       "      <td>-0.064066</td>\n",
       "      <td>-0.003312</td>\n",
       "      <td>0.042494</td>\n",
       "      <td>-0.070648</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>-0.035684</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>0.133140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017461</td>\n",
       "      <td>0.024403</td>\n",
       "      <td>0.084630</td>\n",
       "      <td>-0.006470</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>-0.080750</td>\n",
       "      <td>-0.051471</td>\n",
       "      <td>-0.020571</td>\n",
       "      <td>-0.035888</td>\n",
       "      <td>0.032818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59a85aceb18b146ddb84ff2c</th>\n",
       "      <td>-0.005425</td>\n",
       "      <td>-0.018609</td>\n",
       "      <td>-0.018961</td>\n",
       "      <td>0.051410</td>\n",
       "      <td>-0.066882</td>\n",
       "      <td>0.033543</td>\n",
       "      <td>0.042023</td>\n",
       "      <td>-0.053016</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.042850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031156</td>\n",
       "      <td>-0.029165</td>\n",
       "      <td>0.022447</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>-0.008506</td>\n",
       "      <td>-0.059322</td>\n",
       "      <td>-0.005609</td>\n",
       "      <td>0.033460</td>\n",
       "      <td>-0.006075</td>\n",
       "      <td>-0.038163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59a85aceb18b146ddc84ff2b</th>\n",
       "      <td>-0.001685</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>-0.025039</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>-0.024010</td>\n",
       "      <td>-0.006751</td>\n",
       "      <td>-0.016207</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>-0.008480</td>\n",
       "      <td>-0.013157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018060</td>\n",
       "      <td>-0.011136</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>-0.002854</td>\n",
       "      <td>-0.012020</td>\n",
       "      <td>-0.060105</td>\n",
       "      <td>-0.014063</td>\n",
       "      <td>0.047626</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>-0.005372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59a85acfb18b146dda84ff2b</th>\n",
       "      <td>-0.024034</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>-0.034685</td>\n",
       "      <td>0.054530</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.030094</td>\n",
       "      <td>-0.043227</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.027879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030182</td>\n",
       "      <td>-0.015533</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.008071</td>\n",
       "      <td>0.011580</td>\n",
       "      <td>-0.025270</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>0.046426</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>-0.005510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59a85acfb18b146ddb84ff2d</th>\n",
       "      <td>-0.011807</td>\n",
       "      <td>-0.010629</td>\n",
       "      <td>-0.053721</td>\n",
       "      <td>0.076901</td>\n",
       "      <td>-0.047250</td>\n",
       "      <td>0.013618</td>\n",
       "      <td>0.006365</td>\n",
       "      <td>-0.056048</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037469</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>-0.033402</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>-0.059306</td>\n",
       "      <td>-0.014925</td>\n",
       "      <td>0.034908</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.023625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0         1         2         3         4    \\\n",
       "59a85acdb18b146ddb84ff2b  0.016841 -0.064066 -0.003312  0.042494 -0.070648   \n",
       "59a85aceb18b146ddb84ff2c -0.005425 -0.018609 -0.018961  0.051410 -0.066882   \n",
       "59a85aceb18b146ddc84ff2b -0.001685  0.008268 -0.025039  0.037200 -0.024010   \n",
       "59a85acfb18b146dda84ff2b -0.024034 -0.005111 -0.034685  0.054530 -0.000676   \n",
       "59a85acfb18b146ddb84ff2d -0.011807 -0.010629 -0.053721  0.076901 -0.047250   \n",
       "\n",
       "                               5         6         7         8         9    \\\n",
       "59a85acdb18b146ddb84ff2b  0.006753  0.012448 -0.035684  0.024778  0.133140   \n",
       "59a85aceb18b146ddb84ff2c  0.033543  0.042023 -0.053016  0.002348  0.042850   \n",
       "59a85aceb18b146ddc84ff2b -0.006751 -0.016207 -0.033398 -0.008480 -0.013157   \n",
       "59a85acfb18b146dda84ff2b  0.011178  0.030094 -0.043227  0.000945  0.027879   \n",
       "59a85acfb18b146ddb84ff2d  0.013618  0.006365 -0.056048 -0.002637  0.019908   \n",
       "\n",
       "                            ...          290       291       292       293  \\\n",
       "59a85acdb18b146ddb84ff2b    ...    -0.017461  0.024403  0.084630 -0.006470   \n",
       "59a85aceb18b146ddb84ff2c    ...    -0.031156 -0.029165  0.022447  0.004970   \n",
       "59a85aceb18b146ddc84ff2b    ...    -0.018060 -0.011136  0.006381 -0.002854   \n",
       "59a85acfb18b146dda84ff2b    ...    -0.030182 -0.015533  0.004545  0.008071   \n",
       "59a85acfb18b146ddb84ff2d    ...    -0.037469 -0.004816  0.004341 -0.033402   \n",
       "\n",
       "                               294       295       296       297       298  \\\n",
       "59a85acdb18b146ddb84ff2b  0.001895 -0.080750 -0.051471 -0.020571 -0.035888   \n",
       "59a85aceb18b146ddb84ff2c -0.008506 -0.059322 -0.005609  0.033460 -0.006075   \n",
       "59a85aceb18b146ddc84ff2b -0.012020 -0.060105 -0.014063  0.047626  0.010630   \n",
       "59a85acfb18b146dda84ff2b  0.011580 -0.025270 -0.019287  0.046426  0.034357   \n",
       "59a85acfb18b146ddb84ff2d  0.007919 -0.059306 -0.014925  0.034908  0.027748   \n",
       "\n",
       "                               299  \n",
       "59a85acdb18b146ddb84ff2b  0.032818  \n",
       "59a85aceb18b146ddb84ff2c -0.038163  \n",
       "59a85aceb18b146ddc84ff2b -0.005372  \n",
       "59a85acfb18b146dda84ff2b -0.005510  \n",
       "59a85acfb18b146ddb84ff2d  0.023625  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.lingbuzz\n",
    "papers = db.get_collection('papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = db.get_collection('keywords')\n",
    "sentences = db.get_collection('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debugged\n",
    "def parse_question(sent):\n",
    "    \"\"\"determines whether a word is English/author\"\"\"\n",
    "    sentence = []\n",
    "    author = []\n",
    "    for w in str(sent).split():\n",
    "        w = str(w)\n",
    "        if w.lower() in authors:\n",
    "            author.append(str(w[1:]))\n",
    "        else: \n",
    "            try:\n",
    "                w.encode(encoding='utf-8').decode('ascii')\n",
    "                word = re.sub('[%s]' % re.escape(string.punctuation), '', w)\n",
    "                if word not in standard_stopwords:\n",
    "                    sentence.append(word.lower())\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "    return author, bigrams[sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_question(question):\n",
    "    a, q = parse_question(question)\n",
    "    if 'who' in q or ('which' and 'author') in q:\n",
    "        return request_reference(q[1:])\n",
    "    elif len(a) != 0:\n",
    "        return create_summary(q[1:], a, restrict = True)\n",
    "    else:\n",
    "        return create_summary(q[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debugged\n",
    "# def request_reference(q):\n",
    "#     \"\"\"question is a list of strings\"\"\"\n",
    "#     # for this to work, the keyword entry has to be englishified. \n",
    "#     # candidates = []\n",
    "#     print('These are some papers you might want to read:')\n",
    "#     for candidate in papers.find({'updated_keywords': [q]}):\n",
    "#         print(candidate['title'] + ', by '+ ', '.join(c for c in candidate['authors']))\n",
    "#         print('You can download the paper here: ling.auf.net/' + candidate['url'] + '\\n')\n",
    "        \n",
    "def request_reference(q):\n",
    "    \"\"\"question is a list of strings\"\"\"\n",
    "    # for this to work, the keyword entry has to be englishified. \n",
    "    # candidates = []\n",
    "    answer = 'These are some papers you might want to read: \\n\\n'\n",
    "    for w in q:\n",
    "        for candidate in papers.find({'updated_keywords': w}):\n",
    "            answer += candidate['title'] + ', by '+ ', '.join(c for c in candidate['authors']) + '\\n'\n",
    "            answer += 'You can download the paper here: ling.auf.net/' + candidate['url'] + '\\n\\n'\n",
    "    return(answer)\n",
    "\n",
    "# debugged\n",
    "def restrict_search(a, q):\n",
    "    paperIDs = []\n",
    "    #(?=.*word1)(?=.*word2)(?=.*word3)\n",
    "    regex = '|'.join(a)\n",
    "    for candidate in papers.find({'authors': {'$regex': regex}, 'updated_keywords': {'$in': q}}):\n",
    "        paperIDs.append(candidate['_id'])\n",
    "    return paperIDs\n",
    "\n",
    "# add the paperIDs restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, q = parse_question('what do greco and haegeman say about verb second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectId('598b44c407d7df0771938487')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restrict_search(a, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_Common(lst):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only do this if there are candidates. Otherwise: print I do not have enough information on that. \n",
    "\n",
    "# def create_summary(q, a = None, restrict = False):\n",
    "#     candidates = find_candidates(q)\n",
    "#     out = str()\n",
    "#     if restrict:\n",
    "#         ids = restrict_search(a, q)\n",
    "#         candidates = restrict_candidates(candidates, ids)\n",
    "#         out = '\\nThese are some papers you might want to read: \\n\\n'\n",
    "#         for candidate in papers.find({'_id': {'$in': ids}}):\n",
    "#             out += candidate['title'] + ', by '+ ', '.join(c for c in candidate['authors']) + '\\n'\n",
    "#             out += 'You can download the paper here: ling.auf.net/' + candidate['url'] + '\\n\\n'\n",
    "#     else:\n",
    "#         candidates = restrict_candidates(candidates)\n",
    "#     stack = []\n",
    "#     if len(candidates) == 0 and len(out) == 0:\n",
    "#         print('I do not have enough information to answer that question.')\n",
    "#     if len(candidates) > 0:\n",
    "#         sentence_similarities = calculate_similarities(candidates)\n",
    "#         to_eliminate = []\n",
    "#         while (len(stack) < 5 and len(candidates)>1):\n",
    "#             freqs = most_Common(candidates)\n",
    "#             if freqs[0][1] != freqs[1][1]:\n",
    "#                 stack.append(sentences.find_one({'_id': freqs[0][0]})['sentence'])\n",
    "#                 to_eliminate = [freqs[0][0]] + sentence_similarities[freqs[0][0]]\n",
    "#                 candidates = list(filter(lambda a: a not in to_eliminate, candidates))\n",
    "#             else: \n",
    "#                 sub_candidates = [_id[0] for _id in freqs if _id[1] == freqs[0][1]]\n",
    "#                 sents = sentences.find({'_id': {'$in': sub_candidates}}).sort([('score',-1)]).limit(1)\n",
    "#                 for sent in sents:\n",
    "#                     stack.append(sent['sentence'])\n",
    "#                     to_eliminate+= [sent['_id']] + sentence_similarities[sent['_id']]\n",
    "#                     candidates = list(filter(lambda a: a not in to_eliminate, candidates))\n",
    "#         print(' '.join(stack))\n",
    "#     if len(candidates) ==1:\n",
    "#         #stack.append(sentences.find_one({'_id': candidates[0]})['sentence'])\n",
    "#         print(sentences.find_one({'_id': candidates[0]})['sentence'])\n",
    "#     print(out)\n",
    "#       \n",
    "# def create_summary(q, a = None, restrict = False):\n",
    "#     candidates = find_candidates(q)\n",
    "#     out = str()\n",
    "#     if restrict:\n",
    "#         ids = restrict_search(a, q)\n",
    "#         candidates = restrict_candidates(candidates, ids)\n",
    "#         out = '\\nThese are some papers you might want to read: \\n\\n'\n",
    "#         for candidate in papers.find({'_id': {'$in': ids}}):\n",
    "#             out += candidate['title'] + ', by '+ ', '.join(c for c in candidate['authors']) + '\\n'\n",
    "#             out += 'You can download the paper here: ling.auf.net/' + candidate['url'] + '\\n\\n'\n",
    "#     else:\n",
    "#         candidates = restrict_candidates(candidates)\n",
    "#     stack = []\n",
    "#     if len(candidates) == 0 and len(out) == 0:\n",
    "#         return 'I do not have enough information to answer that question.'\n",
    "#     if len(candidates) > 0:\n",
    "#         sentence_similarities = calculate_similarities(candidates)\n",
    "#         to_eliminate = []\n",
    "#         while (len(stack) < 5 and len(candidates)>1):\n",
    "#             freqs = most_Common(candidates)\n",
    "#             if freqs[0][1] != freqs[1][1]:\n",
    "#                 stack.append(sentences.find_one({'_id': freqs[0][0]})['sentence'])\n",
    "#                 to_eliminate = [freqs[0][0]] + sentence_similarities[freqs[0][0]]\n",
    "#                 candidates = list(filter(lambda a: a not in to_eliminate, candidates))\n",
    "#             else: \n",
    "#                 sub_candidates = [_id[0] for _id in freqs if _id[1] == freqs[0][1]]\n",
    "#                 sents = sentences.find({'_id': {'$in': sub_candidates}}).sort([('score',-1)]).limit(1)\n",
    "#                 for sent in sents:\n",
    "#                     stack.append(sent['sentence'])\n",
    "#                     to_eliminate+= [sent['_id']] + sentence_similarities[sent['_id']]\n",
    "#                     candidates = list(filter(lambda a: a not in to_eliminate, candidates))\n",
    "#         out = ' '.join(stack)+'\\n\\n'+out\n",
    "#     if len(candidates) ==1:\n",
    "#         #stack.append(sentences.find_one({'_id': candidates[0]})['sentence'])\n",
    "#         out += sentences.find_one({'_id': candidates[0]})['sentence']\n",
    "#     return out \n",
    "\n",
    "def create_summary(q, a = None, restrict = False):\n",
    "    candidates = find_candidates(q)\n",
    "    out = str()\n",
    "    refs = str()\n",
    "    if restrict:\n",
    "        ids = restrict_search(a, q)\n",
    "        candidates = restrict_candidates(candidates, ids)\n",
    "        refs = '\\nThese are some papers you might want to read: \\n\\n'\n",
    "        for candidate in papers.find({'_id': {'$in': ids}}):\n",
    "            refs += candidate['title'] + ', by '+ ', '.join(c for c in candidate['authors']) + '\\n'\n",
    "            refs += 'You can download the paper here: ling.auf.net/' + candidate['url'] + '\\n\\n'\n",
    "    else:\n",
    "        candidates = restrict_candidates(candidates)\n",
    "    stack = []\n",
    "    # if len(candidates) == 0 and len(out) == 0:\n",
    "    #     return 'I do not have enough information to answer that question.'\n",
    "    if len(candidates) > 0:\n",
    "        sentence_similarities = calculate_similarities(candidates)\n",
    "        to_eliminate = []\n",
    "        while (len(stack) < 5 and len(candidates)>1):\n",
    "            freqs = most_Common(candidates)\n",
    "            if freqs[0][1] != freqs[1][1]:\n",
    "                to_append = clean_sentence(sentences.find_one({'_id': freqs[0][0]})['sentence'])\n",
    "                if len(to_append.split()) > 3:\n",
    "                    stack.append(to_append)\n",
    "                    to_eliminate = [freqs[0][0]] + sentence_similarities[freqs[0][0]]\n",
    "                else: \n",
    "                    to_eliminate = [freqs[0][0]]\n",
    "                candidates = list(filter(lambda a: a not in to_eliminate, candidates))\n",
    "            else: \n",
    "                sub_candidates = [_id[0] for _id in freqs if _id[1] == freqs[0][1]]\n",
    "                sents = sentences.find({'_id': {'$in': sub_candidates}}).sort([('score',-1)]).limit(1)\n",
    "                for sent in sents:\n",
    "                    to_append = clean_sentence(sent['sentence'])\n",
    "                    if len(to_append.split()) > 3:\n",
    "                        stack.append(to_append)\n",
    "                        to_eliminate+= [sent['_id']] + sentence_similarities[sent['_id']]\n",
    "                    else:\n",
    "                        to_eliminate+= [sent['_id']]\n",
    "                    candidates = list(filter(lambda a: a not in to_eliminate, candidates))\n",
    "        out = ' '.join(stack)+'\\n'\n",
    "    if len(candidates) ==1:\n",
    "        out += clean_sentence(sentences.find_one({'_id': candidates[0]})['sentence']) \n",
    "    if len(candidates) == 0 and len(out) == 0:\n",
    "         out = 'I do not have enough information to answer that question.'\n",
    "    out += '\\n\\n'+ refs\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_similarities(candidates):\n",
    "    sub_df = df.filter(items=list(set(candidates)), axis = 0)\n",
    "    cos_sim = cosine_similarity(sub_df.values)\n",
    "    df_sim = pd.DataFrame(cos_sim, index = sub_df.index, columns = sub_df.index)\n",
    "    similar_sent = {}\n",
    "    for k in candidates:\n",
    "        indexes = list(df_sim[k][df_sim[k]>0.92].index)\n",
    "        if len(indexes) > 0:\n",
    "            similar_sent[k] = indexes\n",
    "    return similar_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debugged\n",
    "\n",
    "# def find_candidates(q):\n",
    "#     candidates = []\n",
    "#     #similar_words = []\n",
    "#     for w in q:\n",
    "#         try: \n",
    "#             candidates+=keywords.find_one({'word': w})['sentenceIDs']      \n",
    "#             #similar_words+=keywords.find_one({'word': w})['similar_words']\n",
    "#         except: \n",
    "#             pass\n",
    "#     #for w in similar_words:\n",
    "#     #    candidates+=keywords.find_one({'_id': w})['sentenceIDs']\n",
    "#     return candidates\n",
    "\n",
    "\n",
    "def find_candidates(q):\n",
    "    candidates = []\n",
    "    #similar_words = []\n",
    "    for candidate in keywords.find({'word': {'$in': q}}):\n",
    "            #similar_words+=keywords.find_one({'word': w})['similar_words']\n",
    "        candidates+=(candidate['sentenceIDs'] )\n",
    "    #for w in similar_words:\n",
    "    #    candidates+=keywords.find_one({'_id': w})['sentenceIDs']\n",
    "    return candidates\n",
    "\n",
    "# def find_candidates(q):\n",
    "#     candidates = []\n",
    "#     similar_words = []\n",
    "#     frequencies = {}\n",
    "#     for w in q:\n",
    "#         frequencies[w]= keywords.find_one({'word': w})['frequency']\n",
    "#     least_frequent = min(frequencies, key=frequencies.get)\n",
    "#     q.remove(least_frequent)\n",
    "#     try: \n",
    "#         candidates+=keywords.find_one({'word': least_frequent})['sentenceIDs']      \n",
    "#         similar_words+=keywords.find_one({'word': w})['similar_words']\n",
    "#     except: \n",
    "#         pass\n",
    "#     for w in q:\n",
    "#         try: \n",
    "#             for _id in keywords.find_one({'word': least_frequent})['sentenceIDs']:\n",
    "#                 if _id in candidates: \n",
    "#                     candidates.append(_id)   \n",
    "#                 similar_words+=keywords.find_one({'word': w})['similar_words']\n",
    "#         except: \n",
    "#             pass\n",
    "#     for w in similar_words:\n",
    "#         try: \n",
    "#             for _id in keywords.find_one({'word': least_frequent})['sentenceIDs']:\n",
    "#                 if _id in candidates: \n",
    "#                     candidates.append(_id)   \n",
    "#                 similar_words+=keywords.find_one({'word': w})['similar_words']\n",
    "#         except: \n",
    "#             pass\n",
    "#     return candidates\n",
    "# \n",
    "# gets rid of the sentences with a low score and takes into account restricted search\n",
    "# def restrict_candidates(candidates, ids = None):\n",
    "#     print('restricting candidates')\n",
    "#     print(len(candidates))\n",
    "#     for c in candidates:\n",
    "#         if ids:\n",
    "#             try: \n",
    "#                 if sentences.find_one({'_id': c})['paperID'] not in ids:\n",
    "#                     candidates.remove(c)\n",
    "#                 if sentences.find_one({'_id': c})['score'] <= 0.7:\n",
    "#                     candidates.remove(c)\n",
    "#             except:\n",
    "#                 pass\n",
    "#         else:\n",
    "#             try: \n",
    "#                 if sentences.find_one({'_id': c})['score'] <= 0.7:\n",
    "#                     candidates.remove(c)\n",
    "#             except:\n",
    "#                 pass\n",
    "#     print(len(candidates))\n",
    "#     return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this still takes too long... As far as I could tell, it is not possible to return mongo queries as a list in python\n",
    "# there is a javascript function that does this, but no equivalent in python. And in any case, it involves looping \n",
    "# as well\n",
    "\n",
    "def restrict_candidates(candidates, ids = None):\n",
    "    print('found %s candidates' %len(candidates))\n",
    "    if ids:\n",
    "        for doc in sentences.find({'_id': {'$in': candidates}, 'paperID': {'$nin': ids}}):\n",
    "            candidates = list(filter(lambda x: x != doc['_id'], candidates))\n",
    "        print('restricted to %s candidates based on ids' %len(candidates))\n",
    "    for doc in sentences.find({'_id': {'$in': candidates}, 'score': {'$lt': 0.5}}):\n",
    "        candidates = list(filter(lambda x: x != doc['_id'], candidates))\n",
    "    print('restricted to %s candidates' %len(candidates))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 0 candidates\n",
      "restricted to 0 candidates\n",
      "I do not have enough information to answer that question.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_question('What is balooba?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sent):\n",
    "    sent = sent.split('  ')\n",
    "    out = str()\n",
    "    for s in sent:\n",
    "        s = s.lstrip('0123456789.- ')\n",
    "        try: \n",
    "            if s[0].isupper():\n",
    "                if len(s)> len(out):\n",
    "                    out = s.strip()\n",
    "            else:\n",
    "                if out[-1] != '.':\n",
    "                    out+= ' ' +s.strip()\n",
    "        except:\n",
    "            pass\n",
    "    try: \n",
    "        if out[-1] not in ['.', '?', '!']:\n",
    "            out = ''\n",
    "    except:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "\n",
    "def clean_sentence1(sent):\n",
    "    \"\"\"helper function to clean sentences up a bit\"\"\"\n",
    "    sent = sent.split('   ')\n",
    "    out = str()\n",
    "    for s in sent:\n",
    "        s = s.lstrip('0123456789.- ').replace('- ', '')\n",
    "        try: \n",
    "            if s[0].isupper():\n",
    "                if len(s)> len(out):\n",
    "                    out = s\n",
    "            else:\n",
    "                if out[-1] != '.':\n",
    "                    out+= ' ' +s\n",
    "        except:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "# def clean_sentence(sent):\n",
    "#     \"\"\"helper function to clean sentences up a bit\"\"\"\n",
    "#     return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 'zan  ’t   I’ll   it      zeggen.  tell   1  In this paper, we discuss a specific set of Verb Third phenomena in West Flemish, the  dialect spoken in the Belgian province of West Flanders, exploring the microvariation  with Standard Dutch and the ramifications that these patterns have for the interaction  between discourse and the syntax of V2 and for the interaction between discourse and  the narrow syntax in general.   '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = 'Tense sufﬁxes come in many forms, but an important generalization exists that helps to elucidate the pat- terns found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2 = 'A can occur either in the first or in the second position  A can only occur in the second position  A linearly precedes B  phi-features (person/number/gender)  focus feature  operator feature  question feature  negation  existential quantification  first person  second person  third person  accusative  affirmative particle  complementizer  interrogative complementizer (Japanese and Irish)  complementizer agreement  clitic pronoun  conditional mood (Finnish)  dative  demonstrative pronoun  emphatic form  imperative  infinitive  negative clitic (Dutch dialects)  negative auxiliary (Finnish)  nominative  object  past tense  dummy Case preposition (Romanian)  plural  present tense  particle  preverb (Hungarian)  relative pronoun  singular  strong pronoun  subject  topic marker (Japanese)  weak pronoun x  SG  STRONG  SUBJ  TOP  WEAK a.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = 'Tense sufﬁxes come in many forms, but an important generalization exists that helps to elucidate the pat- terns found.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tense sufﬁxes come in many forms, but an important generalization exists that helps to elucidate the patterns found.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence1(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 18874 candidates\n",
      "restricted to 36 candidates based on ids\n",
      "restricted to 8 candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Verb second, the split CP and null subjects in early Dutch finite from fragments and ellipsis. The nature of Old Spanish Verb Second reconsidered. The second goal is to investigate the role of syntax in these patterns. Verbs and Verb phrases. In this paper, we discuss a specific set of Verb Third phenomena in West Flemish, the dialect spoken in the Belgian province of West Flanders, exploring the microvariation with Standard Dutch and the ramifications that these patterns have for the interaction between discourse and the syntax of V2 and for the interaction between discourse and the narrow syntax in general.\\nAssuming the context given in (14), let us consider examples in which the attitude verb willen (‘to want’) takes an infinitival complement.\\n\\n\\nThese are some papers you might want to read: \\n\\nFramesetters and the micro-variation of subject-initial V2, by Ciro Greco, Liliane Haegeman\\nYou can download the paper here: ling.auf.net//lingbuzz/003226\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_question('what does Haegeman write about verb second?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "too_low = []\n",
    "for candidate in sentences.find({'score':{'$lt': 0.5}}):\n",
    "        too_low.append(candidate['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407306"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(too_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in keywords.find():\n",
    "    keywords.update_one({'_id': word['_id']}, {'$set': {'informative_sents': list(filter(lambda x: x not in too_low, word['sentenceIDs']))}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x not in [1,2,3], [1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('59a85f7cb18b14085d6c5eac')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords have to be split into a list of words in order for the 'request reference' to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_keywords(keywords):\n",
    "    sentence = []\n",
    "    for w in keywords[0].split():\n",
    "        w = str(w).lower()\n",
    "        word = re.sub('[%s]' % re.escape(string.punctuation), '', w)\n",
    "        if word.isalpha():\n",
    "            sentence.append(word)\n",
    "    return bigrams[sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc in papers.find({'updated_keywords':{'$exists': True}}):\n",
    "    try: \n",
    "        papers.update_one({'_id': doc['_id']},{'$set':{'updated_keywords': clean_keywords(doc['keywords'])}})\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers.find({'keywords':{'$exists': True}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers.find({'updated_keywords':{'$exists': True}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to make the search faster:\n",
    "* Start with looking for sentences that contain the least frequent keyword. Then, within that set, find the sentences that have the other keywords as well. The problem with this is that some keywords are more important than others. For instance, in the question 'What is the difference between topic and focus?', we want to look at sentences with topic and focus, while 'difference' is the least frequent word. We should have some keyword relevance ranking system as well.\n",
    "* everytime that sentence similarities are computed, store them in the db. After a while, all similarities should be in there.\n",
    "* Store Q-A pairs in a db, in order to avoid having to recompute an answer to a question that has already been asked.\n",
    "\n",
    "\n",
    "Ideas to make the general answers better:\n",
    "* In English, the scope of the question is usually where default intonation falls, i.e., the last constituent of the clause. Is there a way to incorporate this?\n",
    "* text simplification: now there are a lot of semantic connectors in the answers that do not connect to anything. It would be better to get rid of them.\n",
    "* make the author search better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc in keywords.find():\n",
    "    keywords.update_one({'_id': doc['_id']}, {'$set': {'frequency': len(doc['sentenceIDs'])} })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords.find_one({'word': 'the'})['frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = ['What is the difference between topic and focus?', 'Who wrote about focus?', \n",
    "             'What did Schlenker write about sing language?']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
